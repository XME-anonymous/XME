[2023-08-02 12:56:32,585][__main__][INFO] - 

alg: ft
lr: 1.0e-05
edit_lr: 5.0e-06
seed: 0
debug: false
model_save_pt: 5000
edit_bs: 1
silent: false
max_iters: 1000000
log_interval: 100
val_interval: 5000
lr_lr: 0.001
batch_size: 2
val_batch_size: 5
accumulate_bs: 10
cedit: 0.1
cloc: 1.0
cbase: 1.0
val_steps: 500
device: cuda
base_loss: distill
oracle: false
train: false
train_base: false
opt: Adam
single_batch: false
archive: /home/anonymous-xme/mend/mend/outputs/2023-07-25_16-27-35_2630413402/models/bert-base-multilingual-uncased.2023-07-25_16-27-35_2630413402.bk
grad_clip: 100.0
ref: null
early_stop_patience: 20000
early_stop_key: loss/total_edit_val
dropout: 0.0
tokenizer: null
results_dir: null
no_grad_layers: null
eval_only: true
half: false
save: false
loc_acc: true
wandb_enabled: true
lang_set:
  en: english
  es: spanish
  fr: french
  hi: hindi
  gu: gujarati
  bn: bengali
  kn: kannada
  ml: malayalam
  ta: tamil
  ar: arabic
  zh: chinese
model:
  pt: /home/anonymous-xme/bloom-finetune/results/mbert-uncased/finetuning-fever-1L-kannada-lr15-wt13-pred/checkpoint-9843/pytorch_model.bin
  name: bert-base-multilingual-uncased
  class_name: AutoModelForSequenceClassification
  tokenizer_class: AutoTokenizer
  tokenizer_name: bert-base-multilingual-uncased
  inner_params:
  - bert.encoder.layer.0.intermediate.dense.weight
  - bert.encoder.layer.0.output.dense.weight
  - bert.encoder.layer.1.intermediate.dense.weight
  - bert.encoder.layer.1.output.dense.weight
  - bert.encoder.layer.2.intermediate.dense.weight
  - bert.encoder.layer.2.output.dense.weight
  - bert.encoder.layer.3.intermediate.dense.weight
  - bert.encoder.layer.3.output.dense.weight
data:
  path: null
  rephrase: true
  zsre_nq: true
  nq_path: ${hydra:runtime.cwd}/data/nq
  wiki_webtext: true
  n_edits: 1
eval:
  verbose: true
  log_interval: 100
  final_eval: true
ft:
  verbose: false
  max_edit_steps: 100
  time_limit: null
  locality:
    enabled: false
    oracle: true
    cedit: 0.01
    batch_size: 1
  rank: null
  opt: RMSprop
  init_std: 0.01
task: fc
dataset: fever
train_set: fever/old-dataset/fever_train_1200 - english_1200.jsonl
val_set: fever/experiment/fever_dev_1200 - english.jsonl
tests: true
edit_lang: english


[2023-08-02 12:56:32,585][__main__][INFO] - Project base directory: /home/anonymous-xme/mend/mend
[2023-08-02 12:56:32,690][models][INFO] - Loading model class <class 'transformers.models.auto.modeling_auto.AutoModelForSequenceClassification'> with name bert-base-multilingual-uncased from cache dir /home/anonymous-xme/mend/mend/cache/
[2023-08-02 12:56:35,434][models][INFO] - Loading model initialization from /home/anonymous-xme/bloom-finetune/results/mbert-uncased/finetuning-fever-1L-kannada-lr15-wt13-pred/checkpoint-9843/pytorch_model.bin
1. Model config:  BertConfig {
  "_name_or_path": "bert-base-multilingual-uncased",
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "directionality": "bidi",
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "position_embedding_type": "absolute",
  "transformers_version": "4.26.1",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 105879
}










[2023-08-02 12:56:36,020][models][INFO] - Loaded model initialization
[2023-08-02 12:56:36,022][models][INFO] - Set 38 dropout modules to p=0.0
Data Size:  1195
Data Size:  1200
[2023-08-02 12:56:36,780][__main__][INFO] - Loading class FT from module <module 'algs.ft' from '/home/anonymous-xme/mend/mend/algs/ft.py'>
[2023-08-02 12:56:42,837][utils][INFO] - Loading checkpoint from /home/anonymous-xme/mend/mend/outputs/2023-07-25_16-27-35_2630413402/models/bert-base-multilingual-uncased.2023-07-25_16-27-35_2630413402.bk
[2023-08-02 12:56:43,716][utils][INFO] - Load complete.
Index file saved in ./run_our_experiments/index_list/
Evaluation started from index kannada mbert-uncased-kannada-init-layers-1 to kannada mbert-uncased-kannada-init-layers-1 in CUDA 3
Total number of experiments: 1
======================================== english ========================================
CUDA_VISIBLE_DEVICES=3 python -m run +alg=ft ++loc_acc=True +experiment=fc +model=mbert-uncased-kannada-init-layers-1 ++archive=/home/anonymous-xme/mend/mend/outputs/2023-07-25_16-27-35_2630413402/models/bert-base-multilingual-uncased.2023-07-25_16-27-35_2630413402.bk +train_set="fever/old-dataset/fever_train_1200 - english_1200.jsonl" +val_set="fever/experiment/fever_dev_1200 - english.jsonl" +tests=True  ++edit_lang=english| tee logs-locality/ft_loc_mbert-uncased_finetuned_kannada_init_layers_1/english.txt
